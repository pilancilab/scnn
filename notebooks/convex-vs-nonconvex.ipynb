{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "750474ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Compare convex and non-convex optimization for a realizable classification problem.\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6284d725",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "\n",
    "from scnn.private.utils.data import gen_classification_data\n",
    "\n",
    "from scnn.optimize import optimize\n",
    "from scnn.regularizers import NeuronGL1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c82ce8",
   "metadata": {},
   "source": [
    "# 1. Generate Data\n",
    "\n",
    "We generate a realizable classification dataset by sampling labels from a two-layer ReLU network with random weights.\n",
    "The follow parameters control the make-up of the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf47351b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate realizable synthetic classification problem (ie. Figure 1)\n",
    "n_train = 1000\n",
    "n_test = 1000\n",
    "d = 25\n",
    "hidden_units = 100\n",
    "kappa = 1000  # condition number\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = gen_classification_data(123, n_train, n_test, d, hidden_units, kappa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0dfe1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(logits, y):\n",
    "    return np.sum((np.sign(logits) == y)) / len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a9d70d",
   "metadata": {},
   "source": [
    "# 2. Non-Convex Training\n",
    "\n",
    "First, we try to trian a neural network using the standard non-convex approach.\n",
    "We use PyTorch to create and differentiate a two-layer neural network with ReLU activations.\n",
    "Note the amount of boiler-plate required to start training such a simple model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a72dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cast data and create loader\n",
    "tX_train, ty_train, tX_test, ty_test = [torch.tensor(z, dtype=torch.float) for z in [X_train, y_train, X_test, y_test]]\n",
    "\n",
    "loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(tX_train, ty_train), batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e81bd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model parameters\n",
    "lam = 0.001\n",
    "\n",
    "# optimization parameters\n",
    "tol = 1e-6 \n",
    "max_epochs = 1000\n",
    "\n",
    "# try playing with the step size...\n",
    "\n",
    "# lr = 0.01\n",
    "# lr = 0.001\n",
    "# lr = 0.0001\n",
    "lr = 0.00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714b25d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "nc_model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(in_features=d, out_features=hidden_units, bias=False), \n",
    "    torch.nn.ReLU(), \n",
    "    torch.nn.Linear(in_features=hidden_units, out_features=1, bias=False))\n",
    "\n",
    "# Acc Before Training\n",
    "print(\"Pre-Training Test Accuracy:\", accuracy(nc_model(tX_test).detach().numpy(), y_test), \"\\n\")\n",
    "\n",
    "\n",
    "sgd = torch.optim.SGD(nc_model.parameters(), lr=lr)\n",
    "\n",
    "for i in range(max_epochs):\n",
    "    for X, y in loader:\n",
    "        nc_model.zero_grad()\n",
    "        l2_penalty = sum([torch.sum(param ** 2) for param in nc_model.parameters()])\n",
    "        obj = torch.sum((nc_model(X) - y) ** 2) / (2 * len(y)) + lam * l2_penalty\n",
    "        obj.backward()\n",
    "        \n",
    "        sgd.step()\n",
    "\n",
    "    # check for convergence\n",
    "    \n",
    "    nc_model.zero_grad()\n",
    "    l2_penalty = sum([torch.sum(param ** 2) for param in nc_model.parameters()])\n",
    "    obj = torch.sum((nc_model(tX_train) - ty_train) ** 2) / (2 * len(y_train)) + lam * l2_penalty\n",
    "    obj.backward()    \n",
    "    grad_norm = sum([torch.sum(param.grad ** 2) for param in nc_model.parameters()])\n",
    "\n",
    "    if grad_norm <= tol:\n",
    "        print(f\"Converged at {i}/{max_epochs}\")\n",
    "        break\n",
    "\n",
    "    if i % 25 == 0:\n",
    "        print(f\"{i}/{max_epochs}: Obj - {obj}, Grad - {grad_norm}\")\n",
    "\n",
    "# Acc After Training\n",
    "print(\"\\nPost-Training Test Accuracy:\", accuracy(nc_model(tX_test).detach().numpy(), y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714dcc36",
   "metadata": {},
   "source": [
    "# 3. Convex Reformulation\n",
    "\n",
    "Instead, we optimize a two-layer neural network with gated ReLU activations using convex optimization.\n",
    "Convexification allows us to use sophisticated optimization methods with convergence and optimality guarantees.\n",
    "Training is easy and requires little-to-no setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20ec14f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# number of activation patterns to use.\n",
    "max_neurons = 1000\n",
    "\n",
    "cvx_model, metrics = optimize(\"gated_relu\", \n",
    "                          max_neurons,\n",
    "                          X_train, \n",
    "                          y_train, \n",
    "                          X_test, \n",
    "                          y_test, \n",
    "                          regularizer=NeuronGL1(lam),\n",
    "                          verbose=True,  \n",
    "                          device=\"cpu\")\n",
    "\n",
    "# Acc After Training\n",
    "print(\"\\n \\n\")\n",
    "print(\"Post-Training Test Accuracy:\", accuracy(cvx_model(X_test), y_test))\n",
    "print(f\"Hidden Layer Size: {cvx_model.parameters[0].shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db36cb4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
